# üìã Configuration Field Categories

## üéØ Purpose

‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å Configuration Fields ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ field ‡πÑ‡∏´‡∏ô:
- **‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô** (Must match training)
- **‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ** (User configurable)
- **‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏°‡∏≠** (Always required)
- **‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** (Fixed per model type)

---

## üîí CRITICAL FIELDS (‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô!)

### Fields ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô Training ‡∏ó‡∏∏‡∏Å Phase

| Field | Description | Example | Why Critical |
|-------|-------------|---------|--------------|
| `input_size` | Input image size (H, W) | `[640, 640]` | Model architecture fixed at this size |
| `input_format` | Color format | `"RGB"` or `"BGR"` | Model trained with specific format |
| `channels` | Number of channels | `3` (RGB) or `1` (Gray) | Model architecture fixed |
| `resize_method` | How to resize input | `"letterbox"` or `"direct"` | Affects feature extraction |
| `padding_color` | Color for padding | `[114, 114, 114]` | Model learned with this background |
| `normalize.mean` | Normalization mean | `[0, 0, 0]` | Model weights scaled accordingly |
| `normalize.std` | Normalization std | `[255, 255, 255]` | Model weights scaled accordingly |

### ‚ö†Ô∏è ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:

```
‚ùå Model ‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
‚ùå Accuracy ‡∏•‡∏î‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å (>20%)
‚ùå ‡∏ï‡πâ‡∏≠‡∏á Retrain Model ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
```

### ‚úÖ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:

```yaml
# YOLOv8 Standard
input_size: [640, 640]
input_format: "RGB"
resize_method: "letterbox"
padding_color: [114, 114, 114]
normalize:
  mean: [0, 0, 0]
  std: [255, 255, 255]

# ResNet ImageNet
input_size: [224, 224]
input_format: "RGB"
resize_method: "center_crop"
normalize:
  mean: [123.675, 116.28, 103.53]
  std: [58.395, 57.12, 57.375]

# CRNN Text Recognition
input_size: [32, 128]
input_format: "Grayscale"
channels: 1
resize_method: "direct"
normalize:
  mean: [0.5]
  std: [0.5]
```

---

## ‚ö†Ô∏è FIXED FIELDS (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)

### Fields ‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö Model Type/Architecture

| Field | Depends On | Examples |
|-------|------------|----------|
| `channels` | Input format | RGB=3, Grayscale=1, RGBA=4 |
| `layout` | Framework | PyTorch=NCHW, TensorFlow=NHWC |
| `dtype` | Training precision | `"float32"`, `"uint8"` |
| `output_format` | Model head | YOLO=xywh, Faster-RCNN=xyxy |

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:

```yaml
# If input_format = "RGB"
channels: 3  # ‚ö†Ô∏è Must be 3

# If input_format = "Grayscale"
channels: 1  # ‚ö†Ô∏è Must be 1

# PyTorch models
layout: "NCHW"  # ‚ö†Ô∏è (Batch, Channels, Height, Width)

# TensorFlow models
layout: "NHWC"  # ‚ö†Ô∏è (Batch, Height, Width, Channels)
```

---

## ‚úÖ CONFIGURABLE FIELDS (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ)

### Fields ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

#### Platform Settings

| Field | Options | Default | Notes |
|-------|---------|---------|-------|
| `platform.target` | rk3588, rk3576, rk3562, rv1109, rv1126, rk1808 | `rk3588` | Match your hardware |
| `platform.sub_platform` | Chip variants | `null` | Usually not needed |

#### Quantization Settings

| Field | Options | Default | Notes |
|-------|---------|---------|-------|
| `quantization.type` | FP16, INT8, UINT8 | `FP16` | FP16=accuracy, INT8=speed |
| `quantization.algorithm` | normal, mmse, kl_divergence | `normal` | mmse=better accuracy |
| `quantization.method` | channel, layer | `channel` | channel=better accuracy |
| `optimization_level` | 0, 1, 2, 3 | `3` | 3=most optimized |

#### Inference Settings

| Field | Range | Default | Notes |
|-------|-------|---------|-------|
| `conf_threshold` | 0.0 - 1.0 | `0.25` | Higher=fewer detections |
| `iou_threshold` | 0.0 - 1.0 | `0.7` | Higher=less NMS filtering |
| `max_detections` | 1 - 1000 | `300` | Maximum output boxes |
| `min_box_size` | 0 - 100 | `10` | Minimum box size (pixels) |

#### Dataset Settings (for INT8)

| Field | Description | Recommended |
|-------|-------------|-------------|
| `dataset.path` | Path to calibration images | `dataset.txt` |
| `dataset.size` | Number of images | 500-1000 |
| `dataset.source` | Source of images | train or val (NOT test) |

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á:

```yaml
# High accuracy (development)
quantization:
  type: "FP16"
  optimization_level: 3

# High performance (production)
quantization:
  type: "INT8"
  algorithm: "mmse"
  method: "channel"
  optimization_level: 3
  dataset:
    path: "dataset.txt"
    size: 1000

# Strict detection (fewer false positives)
postprocessing:
  conf_threshold: 0.5    # Higher
  iou_threshold: 0.45    # Lower (more aggressive NMS)

# Loose detection (catch all)
postprocessing:
  conf_threshold: 0.15   # Lower
  iou_threshold: 0.85    # Higher (less NMS)
```

---

## üìä REQUIRED FIELDS (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏°‡∏≠)

### Fields ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ó‡∏∏‡∏Å Model

#### Model Information

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `model.name` | string | Model name | `"bun_detection"` |
| `model.type` | string | Architecture type | `"YOLOv8"`, `"ResNet"`, `"CRNN"` |
| `model.task` | string | Task type | `"detect"`, `"classify"`, `"segment"` |
| `model.version` | string | Model version | `"1.0.0"` |

#### Class Information

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `classes` | list | Class names | `["person", "car", "dog"]` |
| `num_classes` | int | Number of classes | `3` |

#### Input Information

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `input_size` | [int, int] | Input dimensions | `[640, 640]` |
| `input_format` | string | Color format | `"RGB"` |
| `channels` | int | Channel count | `3` |

---

## üìù OPTIONAL FIELDS (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)

### Fields ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á

| Field | Description | Use Case |
|-------|-------------|----------|
| `model.author` | Model creator | Documentation |
| `model.description` | Model description | Documentation |
| `model.created_date` | Creation date | Version tracking |
| `training.epochs` | Training epochs | Reference |
| `training.batch_size` | Batch size used | Reference |
| `training.optimizer` | Optimizer type | Reference |
| `dataset.train_images` | Training image count | Statistics |
| `dataset.val_images` | Validation image count | Statistics |

---

## üéØ Field Usage by Phase

### Phase 1: Training (PT)

**Required:**
- ‚úÖ All model info fields
- ‚úÖ All input fields
- ‚úÖ All preprocessing fields
- ‚úÖ Class information

**Optional:**
- Training parameters (epochs, batch_size, etc.)
- Dataset statistics

**Forbidden:**
- ONNX-specific fields
- RKNN-specific fields

### Phase 2: Export (ONNX)

**Required:**
- ‚úÖ Copy all from Phase 1 (training fields)
- ‚úÖ ONNX opset version
- ‚úÖ Input/output names and shapes

**Optional:**
- Export tool version
- Simplification settings

**Configurable:**
- Dynamic axes
- Opset version (11-16)

### Phase 3: Conversion (RKNN)

**Required:**
- ‚úÖ Copy all from Phase 2 (training + ONNX fields)
- ‚úÖ Platform target
- ‚úÖ Quantization type

**Optional:**
- Hybrid quantization config
- Custom optimization

**Configurable:**
- ‚úÖ Platform, quantization, optimization
- ‚úÖ Postprocessing thresholds
- ‚úÖ Runtime settings

---

## üö® Validation Rules

### Pre-Conversion Checks

```python
def validate_config(current_phase, previous_phase):
    # Critical fields must match
    assert current['input_size'] == previous['input_size']
    assert current['input_format'] == previous['input_format']
    assert current['resize_method'] == previous['resize_method']
    assert current['padding_color'] == previous['padding_color']
    assert current['normalize'] == previous['normalize']
    
    # Class info must match
    assert current['classes'] == previous['classes']
    assert current['num_classes'] == previous['num_classes']
    
    print("‚úÖ Configuration validated!")
```

### Post-Conversion Checks

```python
def validate_performance(current, baseline):
    # Accuracy should not drop more than 5%
    accuracy_drop = baseline['mAP50'] - current['mAP50']
    assert accuracy_drop < 0.05, f"Accuracy drop too large: {accuracy_drop:.1%}"
    
    # Should have speedup on NPU
    if current['device'] == 'NPU':
        assert current['fps'] > baseline['fps'], "No speedup on NPU"
    
    print("‚úÖ Performance validated!")
```

---

## üìö Quick Reference

### ‚ùå Never Change (Training Config)
- input_size, input_format, channels
- resize_method, padding_color
- normalize.mean, normalize.std

### ‚ö†Ô∏è Fixed (Per Model Type)
- layout (NCHW/NHWC)
- dtype (float32/uint8)
- output_format

### ‚úÖ Always Adjust (Per Hardware)
- platform.target
- quantization settings

### ‚úÖ May Adjust (Per Use Case)
- conf_threshold, iou_threshold
- max_detections, min_box_size
- optimization_level

---

**üìÖ Last Updated:** November 27, 2025  
**üîñ Version:** 1.0.0
